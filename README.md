
# üß† Descripci√≥n General del Proyecto

Este proyecto implementa una soluci√≥n completa de MLOps distribuida en tres servidores, dise√±ada para gestionar todo el ciclo de vida de un modelo de machine learning que predice precios de propiedades inmobiliarias.

La arquitectura del proyecto est√° basada en contenedores Docker orquestados con Kubernetes (MicroK8s) y est√° organizada en tres entornos funcionales independientes, desplegados en m√°quinas virtuales diferentes:

- **Servidor 1**: Encargado del preprocesamiento autom√°tico de datos con Apache Airflow.
- **Servidor 2**: Responsable del registro de experimentos y gesti√≥n de artefactos con MLflow y MinIO.
- **Servidor 3**: Despliega el modelo en producci√≥n mediante una API con FastAPI, integra monitoreo con Prometheus & Grafana, pruebas de carga con Locust y una interfaz de usuario con Streamlit.

Este enfoque modular permite escalar y mantener cada componente de forma independiente, emulando un entorno real de producci√≥n distribuido.

> üí° El objetivo principal es proporcionar predicciones precisas de precios inmobiliarios mediante un sistema MLOps completo, permitiendo a los usuarios obtener estimaciones basadas en caracter√≠sticas de las propiedades.

---

## üóÇÔ∏è Distribuci√≥n del Proyecto por Servidores

Este proyecto fue desarrollado colaborativamente y distribuido en tres m√°quinas virtuales, cada una encargada de un componente clave del flujo de trabajo MLOps. Cada servidor tiene su propio `README.md` con detalles t√©cnicos y operativos espec√≠ficos:

| Servidor | Rol Principal                                   | Enlace al Detalle |
|----------|--------------------------------------------------|-------------------|
| üü¶ Servidor 1 | Preprocesamiento de datos con Airflow           | [Ver README Servidor 1](./Servidor1/README.md) |
| üü© Servidor 2 | Seguimiento de experimentos con MLflow y MinIO  | [Ver README Servidor 2](./Servidor2/README.md) |
| üü• Servidor 3 | Despliegue, monitoreo y pruebas de inferencia   | [Ver README Servidor 3](./Servidor3/README.md) |

Cada una de estas secciones incluye:
- Los contenedores desplegados.
- Los DAGs y notebooks asociados.
- Instrucciones de uso y pruebas.

> üìå **Nota:** Todos los servidores est√°n conectados en red local y comparten el acceso a la base de datos y el almacenamiento distribuido configurado para simular un entorno de producci√≥n real.

---

## üß± Arquitectura General del Proyecto

El proyecto est√° distribuido en **tres servidores (m√°quinas virtuales)** que trabajan de manera coordinada para implementar un pipeline completo de MLOps. Cada servidor aloja componentes espec√≠ficos de la arquitectura, asegurando modularidad, escalabilidad y claridad en la implementaci√≥n.

A continuaci√≥n se presenta el diagrama de la arquitectura general:

![Arquitectura](Servidor3/public/General.png)

### üîπ Servidor 1 ‚Äì Preprocesamiento y Almacenamiento de Datos
- **Airflow**: Orquestaci√≥n de pipelines de preprocesamiento y entrenamiento.
- **Base de Datos MySQL**: Almacena datos en dos capas:
  - `RawData`: Datos crudos separados en train, validation y test.
  - `CleanData`: Datos preprocesados listos para entrenamiento.
- **DAGs**:
  - `realtor_price_model.py`: Preprocesamiento, entrenamiento y registro del modelo de precios inmobiliarios.

### üî∏ Servidor 2 ‚Äì Seguimiento de Experimentos
- **MLflow Tracking Server**: Registro de m√©tricas, par√°metros y artefactos.
- **MinIO**: Almacenamiento compatible con S3 para guardar artefactos de modelos.
- **PostgreSQL Metadata**: Almacena la metadata generada por MLflow.
- Imagen personalizada de MLflow desplegada con dependencias para conectividad segura.

### üî∫ Servidor 3 ‚Äì Despliegue, Observabilidad y Experiencia de Usuario
- **FastAPI**: API de inferencia conectada al modelo en producci√≥n desde MLflow.
- **Streamlit**: Interfaz gr√°fica para realizar predicciones desde la web.
- **Prometheus + Grafana**: Monitoreo del comportamiento de la API:
  - Latencia, uso de memoria, conteo de inferencias.

> üß© Cada componente se despleg√≥ como un contenedor independiente y, cuando est√°n en el mismo cl√∫ster, se conectan mediante redes virtuales internas. Los puertos se exponen a trav√©s de NodePort para que puedan comunicarse por IP con otras m√°quinas. Las IP asignadas por el cl√∫ster a cada servidor garantizan el enrutamiento correcto entre servicios.

---
## üõ†Ô∏è Tecnolog√≠as y Componentes Utilizados

El proyecto se compone de varios microservicios, cada uno desplegado en contenedores independientes, comunicados entre s√≠ dentro de un entorno orquestado con Kubernetes:

- **MLflow**: Gesti√≥n de experimentos y modelos. Conectado a MinIO (artefactos) y MySQL (metadatos).
- **Airflow**: Orquestaci√≥n de pipelines de preprocesamiento y entrenamiento.
- **MinIO**: Almacenamiento local de artefactos, compatible con S3.
- **MySQL**: Bases de datos para RawData, CleanData.
- **PostgreSQL**: Base de datos para metadata de MLflow y Airflow.
- **FastAPI**: API de inferencia del modelo en producci√≥n.
- **Streamlit**: Interfaz gr√°fica para predicciones del modelo.
- **Prometheus + Grafana**: Observabilidad y monitoreo de m√©tricas de inferencia.

## Integraci√≥n Continua

### CI/CD Airflow (workflowservidor1)

Este workflow se activa en cada push a la rama `main` que modifique archivos dentro de `Servidor1/airflow/**` o el `docker-compose.yaml` en `Servidor1/`. Tambi√©n puede lanzarse manualmente con `workflow_dispatch`.

1. **Checkout del repositorio**  
   - Clona todo el c√≥digo para garantizar acceso a los archivos de Airflow y al `docker-compose.yaml`.

2. **Definir `IMAGE_TAG`**  
   - Genera un tag sem√°ntico con la fecha y el n√∫mero de ejecuci√≥n (`YYYYMMDD-runNumber`) y lo almacena en la variable de entorno `IMAGE_TAG`.

3. **Login en Docker Hub**  
   - Usa las credenciales guardadas en los secretos (`DOCKERHUB_USER`, `DOCKERHUB_TOKEN`) para iniciar sesi√≥n.

4. **Build de imagen Airflow (carga local)**  
   - Construye la imagen usando como contexto `Servidor1/airflow`.  
   - Etiqueta la imagen como `${{ secrets.DOCKERHUB_USER }}/airflow:${{ IMAGE_TAG }}` y la carga en el daemon local para pruebas.

5. **Smoke-test: regresi√≥n**  
   - Ejecuta un contenedor con la nueva imagen e importa un peque√±o DataFrame, entrena un modelo de regresi√≥n lineal.  
   - Verifica que la forma de los valores coincida con el DataFrame. Si todo est√° correcto, imprime `‚úÖ Entrenamiento`.  
   - Si falla, detiene el workflow evitando que se publique una imagen defectuosa.

6. **Push de imagen validada a Docker Hub**  
   - Si el smoke-test pasa, recompila la imagen (misma ruta) pero con `push: true` para subirla a Docker Hub usando la etiqueta definida.

7. **Actualizar `Servidor1/.env`**  
   - Crea (o modifica) el archivo `Servidor1/.env`.  
   - Actualiza (o agrega) la variable `REPO_AIRFLOW` con `${{ secrets.DOCKERHUB_USER }}/airflow`.  
   - Actualiza (o agrega) la variable `IMAGE_TAG` con el valor de `${{ IMAGE_TAG }}`.

8. **Commit & Push de `Servidor1/.env`**  
   - Configura el nombre y correo de Git para el commit.  
   - Hace `git pull --rebase --autostash origin main` para integrar cambios remotos.  
   - Si hay modificaciones en `Servidor1/.env`, las commitea con el mensaje `chore: actualizar Servidor1/.env con tag ${{ IMAGE_TAG }}` y las empuja a la rama `main`.  
   - As√≠, el entorno de producci√≥n recoger√° autom√°ticamente la nueva imagen desde la variable `IMAGE_TAG`.

---

### CI/CD Servidor2 (workflowservidor2)

Este workflow se dispara en cada push a `main` que afecte rutas bajo `Servidor2/mlflow/**` o `Servidor2/k8s/**`.

1. **Checkout completo**  
   - Clona todo el repositorio con `fetch-depth: 0` para conservar el historial completo. Esto es necesario al hacer pull/rebase en pasos posteriores.

2. **Definir `IMAGE_TAG`**  
   - Genera un tag sem√°ntico `YYYYMMDD-runNumber` y lo guarda en `IMAGE_TAG`.

3. **Build imagen de MLflow**  
   - Construye la imagen Docker usando el Dockerfile en `Servidor2/mlflow`.  
   - La etiqueta como `${{ secrets.DOCKERHUB_USER }}/mlflow-custom:${{ IMAGE_TAG }}`.

4. **Login a Docker Hub**  
   - Se autentica con las credenciales de Docker Hub guardadas en los secretos.

5. **Push de imagen**  
   - Sube la imagen `mlflow-custom:${{ IMAGE_TAG }}` a Docker Hub.

6. **Actualizar Kustomization**  
   - Ingresa a `Servidor2/k8s` y ejecuta `kustomize edit set image` para apuntar la imagen de MLflow a `${{ secrets.DOCKERHUB_USER }}/mlflow-custom:${{ IMAGE_TAG }}`.

7. **Commit de cambios en `kustomization.yaml`**  
   - Configura nombre y correo de Git.  
   - Hace `git pull --rebase --autostash origin main` para integrar posibles cambios remotos.  
   - Si detecta modificaciones en `kustomization.yaml`, las committea con el mensaje `chore: actualizar imagen mlflow-custom ${{ IMAGE_TAG }}` y las empuja a la rama `main`.  
   - De esta manera, el despliegue en Kubernetes mediante Kustomize usar√° la nueva imagen de MLflow.

---

### CI/CD Servidor3 (workflowservidor3)

Este workflow cubre cuatro componentes (Grafana, Prometheus, Streamlit y FastAPI) y se activa en cada push a `main` que modifique archivos en cualquiera de estas rutas:

- `Servidor3/grafana/**`  
- `Servidor3/prometheus/**`  
- `Servidor3/streamlit/**`  
- `Servidor3/fastapi/**`  
- `Servidor3/k8s/**`

1. **Checkout completo**  
   - Clona todo el repositorio con `fetch-depth: 0` para conservar el historial y permitir pull/rebase.

2. **Definir `IMAGE_TAG`**  
   - Genera el tag sem√°ntico `YYYYMMDD-runNumber` y lo asigna a `IMAGE_TAG`.

3. **Build de im√°genes Docker**  
   - Construye, en un solo paso, las siguientes im√°genes:  
     - `${{ secrets.DOCKERHUB_USER }}/grafana:${{ IMAGE_TAG }}` (contexto: `Servidor3/grafana`)  
     - `${{ secrets.DOCKERHUB_USER }}/prometheus:${{ IMAGE_TAG }}` (contexto: `Servidor3/prometheus`)  
     - `${{ secrets.DOCKERHUB_USER }}/streamlit-app:${{ IMAGE_TAG }}` (contexto: `Servidor3/streamlit`)  
     - `${{ secrets.DOCKERHUB_USER }}/fastapi:${{ IMAGE_TAG }}` (contexto: `Servidor3/fastapi`)

4. **Login en Docker Hub**  
   - Se autentica usando los secretos de Docker Hub.

5. **Push de im√°genes**  
   - Sube las cuatro im√°genes con la etiqueta `${{ IMAGE_TAG }}` a Docker Hub.

6. **Actualizar Kustomization en Kubernetes**  
   - Entra a `Servidor3/k8s` y ejecuta los siguientes comandos para cada servicio:  
     ```bash
     kustomize edit set image ${{ secrets.DOCKERHUB_USER }}/grafana:${{ IMAGE_TAG }}
     kustomize edit set image ${{ secrets.DOCKERHUB_USER }}/prometheus:${{ IMAGE_TAG }}
     kustomize edit set image ${{ secrets.DOCKERHUB_USER }}/streamlit-app:${{ IMAGE_TAG }}
     kustomize edit set image ${{ secrets.DOCKERHUB_USER }}/fastapi:${{ IMAGE_TAG }}
     ```

7. **Commit de cambios en `kustomization.yaml`**  
   - Configura el nombre y correo de Git.  
   - Hace `git pull --rebase --autostash origin main` para integrar cambios remotos.  
   - Si hay modificaciones en `kustomization.yaml`, las agrega al staging, committea con el mensaje `chore: actualizar im√°genes a tag ${{ IMAGE_TAG }}` y las empuja a la rama `main`.  
   - As√≠, al aplicar Kustomize en el cl√∫ster, se desplegar√°n autom√°ticamente las nuevas versiones de Grafana, Prometheus, Streamlit y FastAPI.

---


### Resumen de la l√≥gica general

- Cada workflow reacciona autom√°ticamente a cambios en su carpeta correspondiente dentro de `Servidor1`, `Servidor2` o `Servidor3`.
- Genera un **tag √∫nico** para la imagen basado en la fecha (`YYYYMMDD`) y el n√∫mero de corrida de GitHub Actions (`run_number`).
- **Construye** la imagen Docker para el servicio afectado (Airflow, MLflow, Grafana, Prometheus, Streamlit, FastAPI).
- Se **loguea** en Docker Hub y hace **push** de la imagen reci√©n construida con la etiqueta generada.
- Actualiza el archivo de configuraci√≥n (`.env` o `kustomization.yaml`) que identifica qu√© imagen usar en producci√≥n.
- Hace **commit & push** de ese cambio para que el despliegue en Producci√≥n (ya sea Airflow o Kubernetes) consuma autom√°ticamente la nueva versi√≥n.

De esta forma, cualquier cambio en el c√≥digo fuente desencadena el pipeline de CI/CD, garantizando que la imagen Docker se construya, pruebe m√≠nimamente (en el caso de Airflow) y se despliegue de manera coherente y automatizada.

![image](https://github.com/user-attachments/assets/5f6d5fdb-940b-43a6-8ac6-b13b34dbb3e3)   

## Despliegue Continuo con ArgoCD

En este proyecto utilizamos **ArgoCD** para que, al detectar cambios en GitHub, sincronice autom√°ticamente los manifiestos de Kubernetes con los tres clusters (Servidor1, Servidor2 y Servidor3). Cada ‚ÄúApplication‚Äù de ArgoCD hace lo siguiente:

![image](https://github.com/user-attachments/assets/daa027f7-cfc1-43d2-a95d-61dd4756d227)


- **servidor1-mlops**  
  - Monitorea la carpeta `Servidor1/kubernetes` en la rama `main`.  
  - Al encontrar cambios, aplica los manifiestos en el cluster de Servidor1.  
  - Las im√°genes referenciadas en los YAML se obtienen de Docker Hub.

- **Servidor2-Mlops**  
  - Monitorea la carpeta `Servidor2/k8s` en la rama `main`.  
  - Cada cambio se aplica ‚Äúin-cluster‚Äù en Servidor2 (namespace `mlops`).  
  - Las im√°genes Docker provienen de Docker Hub.

- **servidor3-mlops**  
  - Monitorea la carpeta `Servidor3/k8s` en la rama `main`.  
  - Al detectar actualizaciones, sincroniza los recursos en el cluster de Servidor3.  
  - Tambi√©n descarga las im√°genes desde Docker Hub seg√∫n las etiquetas en los YAML.



Cada vez que se hace push a `main` y hay modificaciones en la carpeta correspondiente, ArgoCD:  
1. Descarga los archivos YAML del repositorio.  
2. Compara con el estado actual del cluster.  
3. Crea, actualiza o elimina recursos para que el cluster coincida con Git.  

De esta forma, los despliegues quedan automatizados y sincronizados con las im√°genes publicadas en Docker Hub.  

## Detalle de cada tarea en el DAG `realtor_price_model`

A continuaci√≥n se describen de manera concisa y concreta las acciones que realiza cada tarea:

![image](https://github.com/user-attachments/assets/54c75d9b-e693-4c9e-b627-1d1ede1c5346)

1. **`extract_data`**  
   - Llama al endpoint `GET /data?group_number=7&day=Tuesday`.  
   - Si la respuesta HTTP es 200 y el JSON contiene filas (`payload["data"]` o lista directa), convierte esas filas en un DataFrame de pandas y las inserta‚Äîcon una marca de tiempo (`fetched_at`)‚Äîen la tabla `realtor_raw` de la base de datos RawData.  
   - Si recibe un error HTTP 400 con el mensaje `"Ya se recolect√≥ toda la informaci√≥n m√≠nima necesaria"`, marca en XCom `finished=True` (fin de datos).  
   - En cualquier caso, registra en XCom dos valores:  
     - `new_records`: n√∫mero de filas nuevas insertadas.  
     - `finished`: booleano que indica si se lleg√≥ al fin de la fuente de datos.

2. **`branch_on_exhaustion`**  
   - Lee `finished` desde XCom (resultado de `extract_data`).  
   - Si `finished=True`, dirige el flujo a la tarea `reset_data`.  
   - Si `finished=False`, dirige el flujo a `decide_to_train`.  
   - Esto se hace mediante un `BranchPythonOperator` que devuelve el `task_id` correspondiente (`"reset_data"` o `"decide_to_train"`).

3. **`reset_data`**  
   - Llama al endpoint `GET /restart_data_generation?group_number=7&day=Tuesday` para reiniciar la generaci√≥n de datos en la fuente externa.  
   - Conecta a las bases RawData y CleanData usando las URIs `AIRFLOW_CONN_MYSQL_DEFAULT` y `AIRFLOW_CONN_MYSQL_CLEAN`.  
   - Inspecciona las tablas en ambas bases y ejecuta `TRUNCATE TABLE` en cada una de estas tablas **solo si existen**:  
     - En RawData: `realtor_raw`, `train`, `validation`, `test`.  
     - En CleanData: `train_clean`, `validation_clean`, `test_clean`.  
   - El prop√≥sito es vaciar por completo los datos hist√≥ricos para comenzar una nueva recolecci√≥n.

4. **`end_after_reset`**  
   - Operador vac√≠o (`EmptyOperator`) que marca el final de la rama ‚Äúreset‚Äù.  
   - Su √∫nica funci√≥n es actuar como punto de convergencia hacia la tarea `end`.

5. **`decide_to_train`**  
   - Recupera `new_records` desde XCom.  
   - Configura MLflow (URI de tracking, experimento `"Realtor_Price_Experiment"`).  
   - Si `new_records == 0`, registra en MLflow m√©tricas y tags (`decision="end_no_train"`, `reason="0 new records"`) y retorna `"end_no_train"` ‚Üí no se entrena.  
   - Si `new_records > 0`, lee las √∫ltimas `new_records` filas de la tabla `realtor_raw` y valida columnas obligatorias (`status`, `bed`, `bath`, `acre_lot`, `house_size`, `prev_sold_date`, `price`):  
     - Si faltan columnas o hay valores nulos o formatos fuera de rango, registra tags en MLflow (`decision="end_no_train"`, raz√≥n de invalidaci√≥n) y retorna `"end_no_train"`.  
     - Si no hay invalidaciones y `new_records > 10000`, registra tags en MLflow (`decision="split_data"`, raz√≥n `"X nuevos > 10000"`) y retorna `"split_data"`.  
     - En cualquier otro caso (`new_records > 0` pero ‚â§ 10000), registra tags en MLflow (`decision="end_no_train"`, raz√≥n `"X nuevos ‚â§ 10000"`) y retorna `"end_no_train"`.  
   - En resumen, decide si conviene ejecutar entrenamiento (solo si hay m√°s de 10 000 registros nuevos y la validaci√≥n preliminar pasa) o finalizar sin entrenar.

6. **`end_no_train`**  
   - Operador vac√≠o (`EmptyOperator`) para la rama donde no se entrena.  
   - Su funci√≥n es actuar como punto de uni√≥n hacia la tarea `end`.

7. **`split_data`**  
   - Conecta a la base RawData (`AIRFLOW_CONN_MYSQL_DEFAULT`) y lee **todo** el contenido de la tabla `realtor_raw`.  
   - Si la tabla est√° vac√≠a, no hace nada y sale.  
   - Si hay datos, los mezcla aleatoriamente (`df.sample(frac=1, random_state=42)`) y reparte en proporciones 60/20/20:  
     - Primer 60 % ‚Üí `train`  
     - Siguiente 20 % ‚Üí `validation`  
     - √öltimo 20 % ‚Üí `test`  
   - Sobrescribe (o crea) las tres tablas en RawData: `train`, `validation` y `test` con los DataFrames correspondientes.  
   - El objetivo es preparar los datos crudos en splits para entrenamiento y validaci√≥n.

8. **`preprocess_data`**  
   - Lee los tres splits reci√©n generados (`train`, `validation`, `test`) de RawData.  
   - Define internamente la funci√≥n `_prep(df)` que realiza:  
     1. Relleno de valores nulos en columnas num√©ricas (`bed`, `bath`, `acre_lot`, `house_size`, `price`) usando forward fill y luego 0.  
     2. C√°lculo de `days_since_last_sale`: convierte `prev_sold_date` a datetime, resta a la fecha actual UTC y extrae el n√∫mero de d√≠as; nulos pasan a ‚Äì1.  
     3. One-hot encoding de la columna categ√≥rica `status`, eliminando una categor√≠a de referencia.  
     4. Eliminaci√≥n de columnas de alta cardinalidad (`brokered_by`, `street`, `zip_code`, `city`, `state`, `prev_sold_date`, `fetched_at`).  
     5. Reordenar columnas para que `price` quede siempre al final.  
   - Aplica `_prep` a cada split crudo (`train`, `validation`, `test`) y obtiene DataFrames limpios (`train_clean`, `validation_clean`, `test_clean`).  
   - Conecta a la base CleanData (`AIRFLOW_CONN_MYSQL_CLEAN`) y sobrescribe (o crea) las tres tablas limpias: `train_clean`, `validation_clean`, `test_clean`.  
   - De este modo, prepara los datos finales que se usar√°n en el paso de entrenamiento.

9. **`train_and_register`**  
   - Lee `train_clean`, `validation_clean` y `test_clean` desde CleanData.  
   - Separa caracter√≠sticas (`X_*`) y etiqueta (`y_* = price`) para cada split.  
   - Configura MLflow (URI de tracking, experimento `"Realtor_Price_Experiment"`).  
   - Define la lista de valores de alpha a probar en un modelo Ridge: `[0.01, 0.1, 1.0, 10.0, 100.0]`.  
   - Construye `all_cols = uni√≥n de todas las columnas presentes en los tres splits` para asegurar consistencia al reindexar.  
   - **Primera fase (b√∫squeda de mejor alpha)**:  
     1. Para cada valor de alpha, entrena sobre `X_train_raw.reindex(columns=all_cols, fill_value=0)` ‚Üí `y_train`.  
     2. Predice sobre `X_val_raw.reindex(columns=all_cols, fill_value=0)` ‚Üí calcula `val_rmse`.  
     3. Registra en MLflow la m√©trica `val_rmse_alpha_{Œ±}`.  
     4. Mantiene el alpha que minimiza `val_rmse` (mejor alpha en validaci√≥n).  
   - **Segunda fase (modelo final)**:  
     1. Concatena `train` + `validation` para reentrenar sobre todo (`df_trval`, `y_trval`).  
     2. Reindexa a `all_cols`, entrena final con `best_alpha`.  
     3. Predice sobre `test` (`X_test_raw.reindex(columns=all_cols, fill_value=0)`), calcula `test_rmse`.  
     4. En el mismo run de MLflow, registra m√©tricas: `best_val_rmse`, `test_rmse` y par√°metro `best_alpha`. Tambi√©n guarda el modelo entrenado en el registro de MLflow como `RealtorPriceModel`.  
   - **Promoci√≥n a producci√≥n**:  
     1. Consulta en MLflow (usando `MlflowClient`) las corridas anteriores excluyendo la actual, ordenadas por `test_rmse` ascendente.  
     2. Si no hay corrida previa o el `test_rmse` actual es mejor que el mejor previo, marca `promoted=True`.  
     3. Reabre el run actual en MLflow y a√±ade tags de orquestaci√≥n (`dag_run_id`, `execution_date`, `previous_best_rmse`, `current_rmse`, `promoted`).  
     4. Si `promoted=True`, transiciona la versi√≥n del modelo reci√©n registrada a **Production** y archiva versiones anteriores en MLflow.  
   - Con esta l√≥gica, se entrena un modelo robusto, se registra en MLflow y solo se promueve si mejora el RMSE en test respecto a todas las ejecuciones anteriores.

10. **`end`**  
    - Operador vac√≠o (`EmptyOperator`) con `trigger_rule=NONE_FAILED_MIN_ONE_SUCCESS`.  
    - Punto de convergencia final en el DAG; se ejecuta si al menos una de las ramas (reset o entrenamiento) finaliz√≥ sin errores.  
    - No realiza ninguna acci√≥n adicional.

---

### Resumen de Flujo

- **Inicio** ‚Üí `extract_data`  
- **Evaluar fin de datos** ‚Üí `branch_on_exhaustion`  
  - Si `finished=True` ‚Üí `reset_data` ‚Üí `end_after_reset` ‚Üí `end`  
  - Si `finished=False` ‚Üí `decide_to_train`  
    - Si decisi√≥n = `"end_no_train"` ‚Üí `end_no_train` ‚Üí `end`  
    - Si decisi√≥n = `"split_data"` ‚Üí `split_data` ‚Üí `preprocess_data` ‚Üí `train_and_register` ‚Üí `end`

De esta forma, el DAG cubre tres caminos principales:

1. **Reset** (cuando se agot√≥ la fuente): se reinicia la generaci√≥n de datos y se vac√≠an tablas.  
2. **No-Train** (si no hay o no alcanza datos nuevos): se termina sin entrenamiento.  
3. **Train** (si hay suficientes datos v√°lidos): divisi√≥n, preprocesamiento, entrenamiento, registro y posible promoci√≥n del modelo.  

## MLflow: Experimento ‚ÄúRealtor_Price_Experiment‚Äù

- Todos los runs (tanto las decisiones como los entrenamientos) se guardan bajo el experimento **Realtor_Price_Experiment**.  
- En la UI de MLflow se ven dos tipos de runs:  
  1. **Decisiones** (‚Äúdecision‚Äù): se registran con tags que indican la raz√≥n (por ejemplo, `decision = end_no_train`, `reason = "0 new records"`).  
  2. **Entrenamientos** (‚Äútrain_manual_YYYY-MM-DD‚Ä¶‚Äù): muestran las m√©tricas (RMSE en validaci√≥n y test), el modelo entrenado (en el Model Registry) y tags de orquestaci√≥n (por ejemplo, `best_alpha`, `current_rmse`, `promoted`).
  
![image](https://github.com/user-attachments/assets/1ee641d1-57e7-49e1-9887-e2a400e81e25)

- Cada run almacena:  
  - **Tags** (razones de decisi√≥n o metadatos del DAG).  
  - **M√©tricas** (solo en entrenamientos).  
  - **Modelo serializado** (solo en entrenamientos) que luego puede marcarse como ‚ÄúProduction‚Äù si mejora el desempe√±o.
 
![image](https://github.com/user-attachments/assets/295aa5bc-d527-4b95-8015-db61dd0dbdc8)

## 3. Observabilidad y Monitorizaci√≥n

Integramos Streamlit, Prometheus y Grafana para ofrecer una vista unificada de predicciones, m√©tricas y dashboards.

### 3.1 Panel de Streamlit (Inferencia y Historial)
- **Objetivo**:
  - Ingresar caracter√≠sticas de una propiedad y obtener la **predicci√≥n estimada** (por ejemplo: `$245,706.06`).
  - Mostrar el **modelo en producci√≥n** (por ejemplo: ‚Äúversi√≥n 9‚Äù).
  - Presentar un **historial tabulado** con todas las corridas del DAG:
    - **Dag_Run_ID**
    - **Decision** (`split_data`, `end_no_train`, etc.)
    - **Decision Reason**
    - **Model name** (solo si se entren√≥)
    - **Model Version** (solo si se promovi√≥)
    - **Current Rsme** y **Previous Rsme** (si aplican)
    - **Promoted** (`true` / `false`)


![image](https://github.com/user-attachments/assets/edd01fa7-6176-4d57-a096-a1d199b3bc11)


![image](https://github.com/user-attachments/assets/bb9e5d3b-913f-4dcb-af99-29742b7c1141)

![image](https://github.com/user-attachments/assets/2f7865d6-9c1c-41f9-9dce-0520c6aa5d88)


### 3.2 Prometheus (Scrape de M√©tricas)
- **Objetivo**:
  - Recopilar m√©tricas de la API de inferencia (FastAPI) y de Prometheus.
  - Configuraci√≥n b√°sica en `prometheus.yml`:

- **M√©tricas Recopiladas**:
  - **Tasa de peticiones de inferencia** (`requests_per_second`)
  - **Latencia de inferencia** (percentil 95: `p95_latency_seconds`)
  - M√©tricas internas de Prometheus (uso de CPU, memoria, etc.)

![image](https://github.com/user-attachments/assets/85e259c3-7c83-432d-9c44-93b36f26c86f)


### 3.3 Grafana (Dashboards de M√©tricas)
- **Objetivo**:
  - Visualizar en tiempo real las m√©tricas recolectadas por Prometheus.
  - Paneles sugeridos:
    - **Tasa de peticiones de inferencia (peticiones/segundo)**
    - **P95 de latencia de inferencias (segundos)**

![image](https://github.com/user-attachments/assets/7bf6bb24-9a03-493e-97a6-e1d7bdcd4d97)



## üöÄ ¬øC√≥mo ejecutar el proyecto completo?
‚úÖ Aseg√∫rate de que los 3 servidores est√©n activos, conectados en la misma red y con Kubernetes (MicroK8s) habilitado.

üîå Paso a paso por servidor
üñ•Ô∏è Servidor 1 ‚Äî Preprocesamiento y orquestaci√≥n

```bash
kubectl apply -f Servidor1/kubernetes/
```
Accede a Airflow y ejecuta el DAG realtor_price_model.py.

üóÉÔ∏è Servidor 2 ‚Äî Almacenamiento y MLflow

```bash
docker build -t custom-mlflow:latest .
docker tag custom-mlflow:latest localhost:32000/custom-mlflow:latest
docker push localhost:32000/custom-mlflow:latest
kubectl apply -f Servidor2/kubernetes/
kubectl apply -f Servidor2/kubernetes/create-minio-bucket.yaml
```

üì° Servidor 3 ‚Äî Inferencia, monitoreo y UI

```bash
kubectl apply -f Servidor3/kubernetes/
```

Accede a la API o interfaz de Streamlit para hacer predicciones.
Verifica m√©tricas en Prometheus y visual√≠zalas en Grafana.
